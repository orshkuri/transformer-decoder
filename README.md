# ğŸ§  Transformerâ€‘based Text Generation

A customâ€‘built **Transformer decoder** model for text generation, trained on **Modern Hebrew** and **Shakespearean English**. This project was implemented entirely from scratch as part of a graduateâ€‘level Deep Learning course, showcasing a deep understanding of attention mechanisms, language modeling, and sequence generation.

---

## ğŸš€ Features

- **Fromâ€‘scratch Transformer decoder** (no HuggingFace, no external frameworks)
- **Multilingual training**: Modern Hebrew and Shakespearean English
- Implements:
  - Causal selfâ€‘attention  
  - Tokenization and padding  
  - Sampling with temperature and topâ€‘k  
  - Checkpointing and resume training
- Utility modules for loss computation, batching, data handling, and visualization

---

## ğŸ—‚ï¸ Project Structure

```text
â”œâ”€â”€ model/                  # Transformer architecture
â”‚   â”œâ”€â”€ transformer.py      # Decoder block & forward pass
â”‚   â”œâ”€â”€ attention.py        # Multiâ€‘head causal selfâ€‘attention
â”‚   â””â”€â”€ mlp.py              # Positionâ€‘wise feedâ€‘forward network
â”œâ”€â”€ utils/                  # Training pipeline & helpers
â”‚   â”œâ”€â”€ data.py             # Dataset loading & preprocessing
â”‚   â”œâ”€â”€ lm.py               # Languageâ€‘model training loop
â”‚   â””â”€â”€ heatmaps.py         # Attentionâ€‘weight visualization
â”œâ”€â”€ configs/                # Hyperâ€‘parameter configs
â”‚   â”œâ”€â”€ config_heb.json     # Hebrew dataset config
â”‚   â””â”€â”€ config_shake.json   # Shakespeare dataset config
â”œâ”€â”€ checkpoints/            # Saved model weights
â”œâ”€â”€ heb-data/               # Hebrew corpus
â”œâ”€â”€ data/                   # Additional corpora
â”‚   â””â”€â”€ input.txt
â”œâ”€â”€ tokenizer_*.json        # Saved tokenizers
â”œâ”€â”€ main.py                 # Entry point for training
â”œâ”€â”€ report.pdf              # Final project report
â”œâ”€â”€ transformers-assignment.pdf  # Course assignment
â””â”€â”€ req.txt                 # Python dependencies
```

---

## ğŸ“¦ Setup

1. **Clone the repository**

   ```bash
   git clone https://github.com/your-username/transformer-textgen.git
   cd transformer-textgen
   ```

2. **Install the requirements**

   ```bash
   pip install -r req.txt
   ```

### ğŸ§° Requirements

Minimal working setup (excerpt from `req.txt`):

```text
torch>=1.10
numpy
```

Optional utilities:

```text
tqdm      # progress bars
```

> ğŸ’¡ *Tip:* After creating your environment, run `pip freeze > req.txt` to save the full package list.

---

## ğŸƒâ€â™‚ï¸ How to Train

Edit `main.py` to choose a dataset:

```python
file_type = "heb"   # or "shake"
```

Then start training:

```bash
python main.py
```

Checkpoints are saved automatically to `./checkpoints/` every *N* steps. Training will resume from the latest checkpoint if one is found.

---

## ğŸ“– Sample Output

### Shakespeare (early epoch)

```text
Shall I compare thee to a summerâ€™s day?
Thou art more lovely and more temperate.
```

### Hebrew (early epoch)

```text
×”×™×•× ×”×–×” ×”×•× ×”×ª×—×œ×” ×—×“×©×” ××œ××” ×‘××•×¨ ×•×ª×§×•×•×”.
```

---

## ğŸ“š Acknowledgments

This project was completed as part of a graduateâ€‘level Deep Learning course. The code emphasizes educational value, reproducibility, and a bottomâ€‘up understanding of Transformer models.

---

## ğŸ§¾ License

This repository is provided for **educational and academic use only**.

---

## ğŸ¤ Contributions

Issues and pull requests are welcome! If you discover bugs or have suggestions, feel free to open an issue or submit a PR.
